---
layout: post
title: CPJC Python Poker Bot
subtitle: How many physicists does it take to know when to hold 'em and when to fold 'em?
thumbnail-img: /img/posts/Images_LongShortPost/GLV_example_curves.png
tags: [Python, Conterfactual Regret Minimization, Texas Hold'em, Algorithm Design]
use_math: true
---

*A dimly lit room, thick with cigarette smoke and the smell of cheap beer. Your opponent across the table sits forward, shoulders hunched, glancing from the cards in his hands to your face and back again. He gingerly pushes half of his stack into the center of the table, like a reluctant church goer when the collection box comes around. You don't even check your own cards before snap shoving every chip you have into the pot. The crest fallen look on his face is all you need to know that you made the right deicision. Your opponent doesn't even look at you as he tosses his cards to the dealer, and you rake in the tumbling mountain of chips to your side of the table. He pleads with you, "At least tell me what you had!?". The corners of your lips turn up slightly, and you reply simply "you have to pay to see 'em".*

<p align="center">
  <img src="../img/posts/Images_PokerBot/Shark_V_Killbot.png" />
</p>
<p align = "center">
Shark versus Killbot, by John Stroud
</p>

**Legal Disclaimer**: The results and techniques discussed below are purely academic in nature and should not be considered as advice. All forms of betting carry financial risk and it is down to the individuals who are making bets to assume responsiblility for any outcomes.

**Gambling problem?**: *Call 1-800-GAMBLER CO, DC, IL, IN, LA, MD, MS, NJ, OH, PA, TN, VA, WV, WY Call 877-8-HOPENY or text HOPENY (467369) (NY) Call 1-800-327-5050 (MA)*

## Forward

This post is intended as a summary of the work that I and the other memebers of the Computational Physics Journal Club at the University of Washington (UW) took on over a roughly 3 month period from February to May in 2024. There were three other contributors not including myself: Micheal Kovtun, Matthew Kafker, and John Stroud. The level of contribution from each person varied with the amount of free time (or urge to procrastinate on real work) that each person had, but the final product is a combination of all of our efforts, and I am incredibly grateful that I was able to work with such a strong, creative team of like minded individuals on an interesting project like this. The topic was born out of a competition that was announced by the Algorithmic Trading Club at UW (https://students.washington.edu/atcuw/hh.html), which was intended to be head to head Poker Bot competition with cash prizes, workshops, and a keynote speaker. Given that the nature of the competition seemed like it would encourage some creative algorithmic and machine learning techniques, it seemed like a natural challenge for a group of technologically minded physics graduate students. Sadly the event never ended up manifesting, and was cancelled due to a lack of participation two days before it was set to be held. Given the time and effort that we put into this project, the news was frustrating to say the least, but we all came away having learned a remarkable amount about the game of poker and all the creative ways that computers have been used to play it. Hopefully this post should give you some idea of what we managed to acomplish over the last 3 months.

## Introduction

In a game like Poker, you are inherently playing with a limited set of information. You don't know what cards your opponent has, or which cards are going to end up being pulled from the deck next. This is in contrast to a game like Chess or Go, where the exact configuration of all the pieces on the board provides complete information to all players. This is of course a dramatic oversimplification of the progress of AI over the last 30 years, but this distinction between 'perfect information' and 'imperfect information' games have led to a divergence in machine learning strategies and levels of sucess when it comes to superhuman (better than the best human players) AI. In the world of AI for games, chess bots like Deep Thought (https://doi.org/10.1007/978-1-4613-9080-0_5) have been able to dominate humans for decades, while the progress of AI in the realm of imperfect information games has lagged significantly in comparison. 

<div align="center">
  <img src="../img/posts/Images_PokerBot/tic-tac-toe.png" alt="tic-tac-toe">
  <p><em>Tic-tac-toe is a "perfect information" game because just by looking at the board both players have full information about the game state. No aspect of play is hidden from either player.</em></p>
</div>

Part of the reason for this comes from the more complicated game state in something like Poker. As already mentioned, in something like tic-tac-toe the path that the players took to get the board into it's current state does not matter at all for figuring out the correct next move. Looking at the image of the tic-tac-toe board, all that matters is where the X's and O's are, and the fact that it's O's turn to go. In Poker however, *the past series of events is just as important as the present state*. This means that the full game state is actually a combination of every decision made up until the point you are currently playing. Let's highlight this with a brief example, but first we need to establish how poker actually works. A full list of rules can be found at https://www.texasholdemonline.com/texas-holdem-rules/ but we'll briefly summarize them here. The game is played with a standard 52 card deck (no jokers) and the goal of the game is to win money from other players. This can be done by either having the strongest 5-card combination or by betting and having all the other players fold. The strength of a 5-card hand is as follows (from best to worst)

**Suits**: Spades (s), Hearts (h), Diamonds (d), Clubs (c)

**Ranks**: Two (2), Three (3), Four (4), Five (5), Six (6), Seven (7), Eight (8), Nine (9), Ten (T), Jack (J), Queen (Q), King (K), Ace (A)

- Royal Flush: (Ace, King, Queen, Jack, Ten) all of the same suit
- Straight Flush: Any 5 cards of the same suit and consecutive rank. Aces are allowed to count as either the highest card or as ones.
- Four of a Kind: Any four cards of the same rank
- Full House: Any three cards from a single rank combined with a pair from a different rank (ex: A Three of a Kind (Js, Jh, Jd) + One Pair (8c, 8s))
- Straigt: Any five cards of consecutive rank
- Three of a Kind: Any three cards of the same rank
- Two pair: Any two pairs of of cards from the same ranks (ex: One Pair (As, Ac) + One Pair (4s, 4d))
- One pair: Any two cards of the same rank
- High Card: Hands that do not fit any of the above categories are ranked based on the highest cards in the hand

Ties within a certain hand rank are often decided by high cards or using the rank of any unused cards. Poker games are broken into hands where you have a set of personal cards (hole cards) and community cards. Each hand is comprised of 4 betting rounds (sometimes called streets). These rounds are pre-flop (no community cards), flop (+3 community cards), turn (+1 community cards, 4 total), and river (+1 community cards, 5 total). Every round, players can either Fold (give up their cards and any money they've bet this round), Check (Indicate that they don't want to bet, which can only be done if no other player has bet this round so far), Call (match the bet of any previous player in the order), and Raise (Bet some amount more than what any other player has previously bet this round). Additionally, there are antees which are known as the small blind and big blind, which are effectively a forced bet that two players have to make at the very beginning of every hand. This servers to keep the game moving by forcing a small amount of skin into the game for one or two players. The blinds will rotate around the entire table multiple times over the course of a normal game to keep things even. Clearly there are a lot of ways that a poker game can shake out and the combinatorics get very nasty very quickly, but that is part of what makes the game challenging. 

![Hypothetical Poker Game](../img/posts/Images_PokerBot/River-Poker-Game.png)

Now getting back to the example, assume that Player A and Player B are having a 1v1 game of poker. It's currently on the river round (last betting round) and Player A has a (Ad, Td), with community cards of {'Flop': (Qs, Ac, 7c), 'Turn': (Tc), 'River': (Qh)}. From the perspective of Player A, if Player B played aggresively in the earlier parts of the game (raising with large bet sizes) and is now playing passively, that may be an indicator that Player B has a single clubs card and didn't end up getting a fourth club from the cummunity cards to make a flush. However, if Player B played passively to begin with (check-calling, never raising) and is now playing aggresively in the late game, it may be an indication that Player B has a Queen, and has made of Three of a Kind with the (Qh) from the river. This of course assumes that Player B is playing 'honestly' or betting according to the strength of their own hand/potential future hand, and not bluffing. What this small example demonstrates however is that they behavior of your opponent (assuming you are Player A) over the course of a hand of poker provides crucial information for understanding the current state of the game (e.g. whether you are likely in a strong or a weak position).


## Creating the bot

All of the code used in this project is available on Github at: https://github.com/mkafker/PokerBot.

In an effort to explore multiple potential strategies we split up the work amongst ourselves. Micheal Kovtun developed a C++ poker engine which could be used for simulating 100,000+ hands of poker per second and allowed us to run large head to head tests with incredibly high throughput. I chose to pursue an approach detailed in academic literature known as Counterfactual Regret Minimization (CFR), which has been used extensively over the last 20 years for coming up with Nash equilibrium strategies to simpler versions of poker and is the basis behind most of the state-of-the-art poker engines today. Matthew Kafker designed a zoo of heuristics based bots which allowed us to rapidly prototype various strategies and test our more sophisticated methods against many different simple but formidable opponents. John Stroud acted as a field tester and helped bounce between the heuristic and CFR bots to determine where the weak points are, and gave advice to Matt and I about how we could improve each of our strategies.

### Counterfactual Regret Minimization

Given that this was the part of the project I directly worked on it is the part that I am most equipt to talk about. Other sections detailing my teammates work will include written excerpts and figures from their own work. If you'd like to learn more about how CFR has historically been used for treating No-limit Texas Hold'em Poker and its simpler variants please refer to the excellent documentation at [aipokertutorial.com](https://aipokertutorial.com/the-cfr-algorithm/).

The CFR algorithm is an iterative self-play approach which is similar to traditional reinforcement learning techniques. The algorithm was first introduced in a 2007 paper by Martin Zinkevich *et al.* [Regret Minimization in Games with Incomplete Information](https://poker.cs.ualberta.ca/publications/NIPS07-cfr.pdf) The goal of the algorithm is to come up with a **strategy** which sets the probability of choosing a particular action for a given game state. An example of this would be "If I have pocket aces (A?, A?) pre-flop, I always go all in" or "If I have 2,7 offsuit (2x, 7y) I will always fold, regardless of which street we're on". Pratically, I find that this is simplest if to visualize if you think about it as a dictionary or an unordered set.

```
master_strategy = {
    "Game State 1" : [0.25, 0.50, 0.25],
    "Game State 2" : [1.00, 0.00, 0.00],
    "Game State 3" : [0.00, 0.01, 0.99],
    "Game State 4" : [1.00, 0.00],
    "Game State 5" : [0.33, 0.34, 0.33],
    "Game State 4" : [0.00, 0.00, 0.00, 1.00],
    ...
}
```

The output of the CFR algorithm is essentially a giant dictionary, where for every full game state (which is defined actions taken so far in the street you are currently player, your hole cards, community cards, and all actions taken in past rounds) there is a corresponding vector with a length that is equal to the number of actions it is possible to take. The values in the vector correspond to the probability of choosing the corresponding action for that index.

For example, the game state for a player that has recieved their hole cards and is first 'under the gun' (immediately after the big blind) in a hand would look something like 

```
{
    "Player Order": 0,
    "street": "pre-flop",
    "Hole Cards": ("As", "Kc"),
    "Community Cards": (), 
    "History": ["BB20"]
}
```
Here the big blind is 20, and therefore player can either Fold, Call for the big blind amount, or Raise to some higher number. Ignoring for the moment the amount associate with the raise, this would produce a strategy vector that looks like $\left[ v_F, v_C, v_R \right]$ where $v_i$ is the probability of taking action $i$ with the condition that strategy vector is normalized, $\sum_i v_i = 1$.

I will leave the mathematics of the CFR algorithm to better educators, but I will explain it as best as I can through figures and code. Building the master strategy dictionary will require walking through the entire game tree and assessing what the best probability distribution over all potential actions for a given game state. The easiest way to "assess" the best choices out of all the possible actions will be using a quantity called **regret**. This is similar to how we as humans learn to make choices when doing a task repeatedly. Let's explore this with a small example.

<div align="center">
  <img src="../img/posts/Images_PokerBot/CFR_bad_example.png" alt="CFR Example">
  <p><em>CFR regret calculation and strategy update example. By looking at the outcomes of an action compare to each other, a regret vector can be calculated, where the "regret" is the difference between the outcomes of each choice and the expectation value of all the outcomes given the probability of choice each action (utility).</em></p>
</div>

Player 1 has hole cards (Ad, Td), Player 2 has hole cards (8c, 9c), and the current street is the turn, so the four community cards are (Ah, Ac, 7c, Tc). Both players started with 1000 and have already contributed 500 to the pot, and Player 1 goes first on the turn and all-ins. This leaves the decision to Player 2 to either call for 500 or fold. Assuming that the probability distribution for Player 2's strategy starts with an equal probability wieght across all possible actions, they will Fold 50% of the time and Call 50% of the time. Looking at the outcome of these particular choices, let's assume that the community card from the river is a (Jc), which is exactly what Player 2 needs to make a straight flush and beat Player 1's full house. This leads to Player 2 winning 1000 net from this hand, as opposed to loosing 500 if they had chosen to fold. This is what goes into the *outcomes* vector. The *utility* is an expectation value of the outcomes based on the probability of choosing the action which led to each outcome, as defined by the *strategy*. Finally, *regrets* are calculated as the difference between the *outcomes* and the *utility*. 

This *regrets* vector will get added into a *regret_avg* vector, where it's contribution will be determined by the probability that this node in the game tree is reached. For the sake of clarity I've ommited this detail from the diagram, but essentially it comes down to the fact that you want some "averaged" regret, where the iterations that affect the average are the ones where it is more likely that this node in the game tree is reached. Think of the extreme case, where an action provides an outcome of winning 10 million dollars, but it is very very unlikely that our opponent would actually all in (given the previous game state). We don't want this data point to skew our average too much, because it's effectively an outlier, and therefore we scale it by the probability that our opponent actually chooses the bad option (as well as some other probabilities that come from the likelyhood of getting delt this specific hole card and community card combination).

Using this *regret_avg* vector, a new strategy can be calculated after each iteration, and it's contribution to the *avg_strategy* will also likelwise be scaled by the probability of reaching this node in the game tree.

